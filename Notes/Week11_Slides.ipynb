{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965b73ad",
   "metadata": {},
   "source": [
    "# Advanced Methods for Improving the Performance of Convolutional Neural Networks\n",
    "\n",
    "CSC/DSC 340 Week 11 Slides\n",
    "\n",
    "Author: [Dr. Julie Butler](http://www.juliebutler.org)\n",
    "\n",
    "Date Created: October 29, 2023\n",
    "\n",
    "Last Modified: October 29, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42522aa5",
   "metadata": {},
   "source": [
    "* Last week we learned how to classify the MNIST data set with convolutional neural networks and how to improve the performance of these CNNs with hyperparameter tuning\n",
    "* This week we will learn how to futher improve the performance using a few preprocessing techniques and a few different architecture styles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e76eb",
   "metadata": {},
   "source": [
    "* First, let's review the MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e8a8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (60000, 28, 28)\n",
      "Train labels shape: (60000,)\n",
      "Test images shape: (10000, 28, 28)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Print the shape of the data\n",
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3ed296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Display a small number of images\n",
    "num_images = 5\n",
    "plt.figure(figsize=(10, 3))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(1, num_images, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(train_labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7739920",
   "metadata": {},
   "source": [
    "* Now let's review how to classify the MNIST data set with a simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d039f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 08:53:43.228942: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 15s 18ms/step - loss: 0.1925 - accuracy: 0.9410 - val_loss: 0.0597 - val_accuracy: 0.9828\n",
      "Epoch 2/5\n",
      "844/844 [==============================] - 15s 18ms/step - loss: 0.0547 - accuracy: 0.9832 - val_loss: 0.0386 - val_accuracy: 0.9888\n",
      "Epoch 3/5\n",
      "844/844 [==============================] - 16s 19ms/step - loss: 0.0378 - accuracy: 0.9881 - val_loss: 0.0389 - val_accuracy: 0.9882\n",
      "Epoch 4/5\n",
      "844/844 [==============================] - 16s 19ms/step - loss: 0.0312 - accuracy: 0.9897 - val_loss: 0.0441 - val_accuracy: 0.9882\n",
      "Epoch 5/5\n",
      "844/844 [==============================] - 16s 19ms/step - loss: 0.0245 - accuracy: 0.9919 - val_loss: 0.0329 - val_accuracy: 0.9910\n",
      "313/313 - 1s - loss: 0.0280 - accuracy: 0.9912 - 1s/epoch - 5ms/step\n",
      "\n",
      "Test accuracy: 0.9911999702453613\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "del train_images\n",
    "del test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d57a5f",
   "metadata": {},
   "source": [
    "* Now let's attempt to improve this accuracy\n",
    "    * NOTE: these techniques may not work well on the MNIST data set since the images are quite simple but will work better with larger and more complex images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563072f2",
   "metadata": {},
   "source": [
    "## Improving Through Pre-Processing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61755f",
   "metadata": {},
   "source": [
    "* First let's increase the size of the data set using _data augmentation_\n",
    "    * Add images to the data set that are rotated, stretched, translated, and zoomed in versions of the original images\n",
    "    * Should lead to better generalization and thus increased accuracy\n",
    "\n",
    "* Note that for the `ImageDataGenerator` function the X data (the images) need to be four dimensions instead of 3 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "426ea7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 08:55:04.444998: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.2924 - accuracy: 0.9078\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.1006 - accuracy: 0.9692\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0724 - accuracy: 0.9778\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.0618 - accuracy: 0.9807\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0526 - accuracy: 0.9836\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0294 - accuracy: 0.9902\n",
      "Test accuracy: 0.9901999831199646\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import scipy.ndimage\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Data preprocessing and normalization\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Create a simple CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model with augmented data\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
    "          steps_per_epoch=len(x_train) / 32, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "del x_train\n",
    "del x_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93365e6",
   "metadata": {},
   "source": [
    "* Next we will attempt to increase the accuracy by sharpening the edges of the digits by removing pixels that are light grey\n",
    "* For more complex images, there are algorithms for edge sharpening, deblurring, etc. that can be applied in a similar manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3298ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Function to remove gray images\n",
    "def remove_gray_images(images, threshold=5):\n",
    "    images_redone = []\n",
    "    for image in images:\n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                if image[i][j] < 255-threshold:\n",
    "                    image[i][j] = 0\n",
    "        images_redone.append(image)\n",
    "    images_redone = np.asarray(images_redone)\n",
    "    return images_redone\n",
    "\n",
    "# Remove gray images from the training set\n",
    "x_train_filtered = remove_gray_images(x_train, 55) # 55 is the number for blackness rating of pixels\n",
    "\n",
    "# Remove gray images from the test set\n",
    "x_test_filtered = remove_gray_images(x_test, 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2fad10",
   "metadata": {},
   "source": [
    "* The original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb853f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Display a small number of images\n",
    "num_images = 5\n",
    "plt.figure(figsize=(10, 3))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(1, num_images, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8287fc",
   "metadata": {},
   "source": [
    "* The sharpened images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f85915f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a small number of images\n",
    "num_images = 5\n",
    "plt.figure(figsize=(10, 3))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(1, num_images, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train_filtered[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53090809",
   "metadata": {},
   "source": [
    "* Now let's train a CNN using the sharpened data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc196a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "844/844 [==============================] - 17s 20ms/step - loss: 0.2083 - accuracy: 0.9354 - val_loss: 0.0671 - val_accuracy: 0.9795\n",
      "Epoch 2/5\n",
      "844/844 [==============================] - 16s 19ms/step - loss: 0.0600 - accuracy: 0.9814 - val_loss: 0.0524 - val_accuracy: 0.9863\n",
      "Epoch 3/5\n",
      "844/844 [==============================] - 17s 20ms/step - loss: 0.0443 - accuracy: 0.9857 - val_loss: 0.0446 - val_accuracy: 0.9868\n",
      "Epoch 4/5\n",
      "844/844 [==============================] - 17s 20ms/step - loss: 0.0329 - accuracy: 0.9892 - val_loss: 0.0403 - val_accuracy: 0.9895\n",
      "Epoch 5/5\n",
      "844/844 [==============================] - 16s 19ms/step - loss: 0.0264 - accuracy: 0.9913 - val_loss: 0.0573 - val_accuracy: 0.9837\n",
      "313/313 - 1s - loss: 0.0571 - accuracy: 0.9837 - 1s/epoch - 4ms/step\n",
      "\n",
      "Test accuracy: 0.9836999773979187\n"
     ]
    }
   ],
   "source": [
    "# Scale the data\n",
    "x_train_filtered = x_train_filtered / 255\n",
    "x_test_filtered = x_test_filtered / 255\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train_filtered, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test_filtered, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "del x_train\n",
    "del x_test\n",
    "del x_train_filtered\n",
    "del x_test_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6bde31",
   "metadata": {},
   "source": [
    "## Improving Accuracy with Differing Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf01402",
   "metadata": {},
   "source": [
    "* First we will test the addition of regularization and drop-out to the CNN\n",
    "    * Regularization: Using L1 or L2 norms to control the values of the weights of the network (like LASSO and Ridge regresion)\n",
    "    * Dropout: randomly setting X% of the weights of a layer to zero\n",
    "* Both methods can reduce overfitting and thus increase accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "702e7f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 11s 22ms/step - loss: 0.8855 - accuracy: 0.8439 - val_loss: 0.5094 - val_accuracy: 0.9335\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.6257 - accuracy: 0.8898 - val_loss: 0.4988 - val_accuracy: 0.9346\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.5766 - accuracy: 0.9038 - val_loss: 0.4411 - val_accuracy: 0.9479\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.5402 - accuracy: 0.9127 - val_loss: 0.4065 - val_accuracy: 0.9573\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.5135 - accuracy: 0.9206 - val_loss: 0.3832 - val_accuracy: 0.9604\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3832 - accuracy: 0.9604\n",
      "Test accuracy: 0.9603999853134155\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = np.expand_dims(x_train, axis=-1) / 255.0\n",
    "x_test = np.expand_dims(x_test, axis=-1) / 255.0\n",
    "\n",
    "# Create a CNN model with regularization and dropout\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "del x_train\n",
    "del x_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d313b6",
   "metadata": {},
   "source": [
    "* Ensemble Learning: Train many CNNs and take the average of the CNNS to be the result\n",
    "    * Also used to reduce overfitting and capture various patterns in the data\n",
    "* The ensemble can be made of CNNs of the same architecture (this example) or different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "693ebd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0447 - accuracy: 0.9841\n",
      "Ensemble test accuracy: 0.9840999841690063\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = np.expand_dims(x_train, axis=-1) / 255.0\n",
    "x_test = np.expand_dims(x_test, axis=-1) / 255.0\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Create multiple CNN models\n",
    "num_models = 5\n",
    "models = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    models.append(model)\n",
    "\n",
    "# Train the models\n",
    "for model in models:\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=5, verbose=0)\n",
    "# Combine predictions from the models\n",
    "\n",
    "y_preds = np.zeros_like(y_test)\n",
    "\n",
    "for model in models:\n",
    "    y_preds += model.predict(x_test)\n",
    "\n",
    "y_preds /= num_models\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_loss, ensemble_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Ensemble test accuracy: {ensemble_acc}')\n",
    "\n",
    "\n",
    "del x_train\n",
    "del x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc9ae9",
   "metadata": {},
   "source": [
    "### More Complex Architecture\n",
    "* Inception Networks, ResNet, DeepNet\n",
    "    * Improve computational effeciency with smaller parallel layers instead of larger layers\n",
    "* Can train the networks using Google's ImageNet weights (transfer learning)\n",
    "* Not useful (in fact not appliciable) the MNIST data set but work well on larger image dta sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d66714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
